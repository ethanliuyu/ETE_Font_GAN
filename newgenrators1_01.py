from torch import nnimport torchfrom einops import rearrangefrom timm.models.layers import DropPath, trunc_normal_from utils.net_init import init_netfrom torch.optim.lr_scheduler import StepLRimport torchvision.utils as vutilsimport osclass PreNorm(nn.Module):    def __init__(self, dim, fn):        super().__init__()        self.ln = nn.LayerNorm(dim)        self.fn = fn    def forward(self, x, **kwargs):        return self.fn(self.ln(x), **kwargs)class FeedForward(nn.Module):    def __init__(self, dim, mlp_dim, dropout):        super().__init__()        self.net = nn.Sequential(            nn.Linear(dim, mlp_dim),            nn.SiLU(),            nn.Dropout(dropout),            nn.Linear(mlp_dim, dim),            nn.Dropout(dropout)        )    def forward(self, x):        return self.net(x)class Attention(nn.Module):    def __init__(self, dim, heads, head_dim, dropout):        super().__init__()        inner_dim = heads * head_dim        project_out = not (heads == 1 and head_dim == dim)        self.heads = heads        self.scale = head_dim ** -0.5        self.attend = nn.Softmax(dim=-1)        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)        self.to_out = nn.Sequential(            nn.Linear(inner_dim, dim),            nn.Dropout(dropout)        ) if project_out else nn.Identity()    def forward(self, x):        qkv = self.to_qkv(x).chunk(3, dim=-1)        q, k, v = map(lambda t: rearrange(t, 'b p n (h d) -> b p h n d', h=self.heads), qkv)        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale        attn = self.attend(dots)        out = torch.matmul(attn, v)        out = rearrange(out, 'b p h n d -> b p n (h d)')        return self.to_out(out)class Transformer(nn.Module):    def __init__(self, dim, depth, heads, head_dim, mlp_dim, dropout=0.):        super().__init__()        self.layers = nn.ModuleList([])        for _ in range(depth):            self.layers.append(nn.ModuleList([                PreNorm(dim, Attention(dim, heads, head_dim, dropout)),                PreNorm(dim, FeedForward(dim, mlp_dim, dropout))            ]))    def forward(self, x):        out = x        for att, ffn in self.layers:            out = out + att(out)            out = out + ffn(out)        return outclass GroupNorm(nn.GroupNorm):    """    Group Normalization with 1 group.    Input: tensor in shape [B, C, H, W]    """    def __init__(self, num_channels, **kwargs):        super().__init__(1, num_channels, **kwargs)class MobileViTAttention(nn.Module):    def __init__(self, in_channel=3, depth=3, dim=512, kernel_size=5, patch_size=2, heads=8, head_dim=64, mlp_dim=1024):        super().__init__()        self.ph, self.pw = patch_size, patch_size        # self.conv1 = nn.Conv2d(in_channel, in_channel, kernel_size=kernel_size, padding=kernel_size // 2)        # self.conv2 = nn.Conv2d(in_channel, dim, kernel_size=1)        self.trans = Transformer(dim=dim, depth=depth, heads=heads, head_dim=head_dim, mlp_dim=mlp_dim)        # self.conv3 = nn.Conv2d(dim, in_channel, kernel_size=1)        #self.conv4 = nn.Conv2d(2 * in_channel, in_channel, kernel_size=1)    def forward(self, x):        y = x.clone()  # bs,c,h,w        ## Local Representation        # y = self.conv2(self.conv1(y))  # bs,dim,h,w        ## Global Representation        _, _, h, w = y.shape        y = rearrange(y, 'bs dim (nh ph) (nw pw) -> bs (ph pw) (nh nw) dim', ph=self.ph, pw=self.pw)  # bs,h,w,dim        y = self.trans(y)        y = rearrange(y, 'bs (ph pw) (nh nw) dim -> bs dim (nh ph) (nw pw)', ph=self.ph, pw=self.pw, nh=h // self.ph,                      nw=w // self.pw)  # bs,dim,h,w        ## Fusion        # y = self.conv3(y)  # bs,dim,h,w        #y = torch.cat([x, y], 1)  # bs,2*dim,h,w        #y = self.conv4(y)  # bs,c,h,w        return y# 具体流程可以参考图1，通道注意力机制class Channel_Att(nn.Module):    def __init__(self, channels, t=16):        super(Channel_Att, self).__init__()        self.channels = channels        self.bn2 = nn.BatchNorm2d(self.channels, affine=True)    def forward(self, x):        residual = x        x = self.bn2(x)        # 式2的计算，即Mc的计算        weight_bn = self.bn2.weight.data.abs() / torch.sum(self.bn2.weight.data.abs())        x = x.permute(0, 2, 3, 1).contiguous()        x = torch.mul(weight_bn, x)        x = x.permute(0, 3, 1, 2).contiguous()        x = torch.sigmoid(x) * residual  #        return xclass LayerNormChannel(nn.Module):    """    LayerNorm only for Channel Dimension.    Input: tensor in shape [B, C, H, W]    """    def __init__(self, num_channels, eps=1e-05):        super().__init__()        self.weight = nn.Parameter(torch.ones(num_channels))        self.bias = nn.Parameter(torch.zeros(num_channels))        self.eps = eps    def forward(self, x):        u = x.mean(1, keepdim=True)        s = (x - u).pow(2).mean(1, keepdim=True)        x = (x - u) / torch.sqrt(s + self.eps)        x = self.weight.unsqueeze(-1).unsqueeze(-1) * x \            + self.bias.unsqueeze(-1).unsqueeze(-1)        return xclass Pooling(nn.Module):    """    Implementation of pooling for PoolFormer    --pool_size: pooling size    """    def __init__(self, pool_size=3):        super().__init__()        self.pool = nn.AvgPool2d(            pool_size, stride=1, padding=pool_size // 2, count_include_pad=False)    def forward(self, x):        return self.pool(x) - xclass Mlp(nn.Module):    """    Implementation of MLP with 1*1 convolutions.    Input: tensor with shape [B, C, H, W]    """    def __init__(self, in_features, hidden_features=None,                 out_features=None, act_layer=nn.GELU, drop=0.):        super().__init__()        out_features = out_features or in_features        hidden_features = hidden_features or in_features        self.fc1 = nn.Conv2d(in_features, hidden_features, 1)        self.act = act_layer()        self.fc2 = nn.Conv2d(hidden_features, out_features, 1)        self.drop = nn.Dropout(drop)        self.apply(self._init_weights)    def _init_weights(self, m):        if isinstance(m, nn.Conv2d):            trunc_normal_(m.weight, std=.02)            if m.bias is not None:                nn.init.constant_(m.bias, 0)    def forward(self, x):        x = self.fc1(x)        x = self.act(x)        x = self.drop(x)        x = self.fc2(x)        x = self.drop(x)        return xclass PoolFormerBlock(nn.Module):    """    Implementation of one PoolFormer block.    --dim: embedding dim    --pool_size: pooling size    --mlp_ratio: mlp expansion ratio    --act_layer: activation    --norm_layer: normalization    --drop: dropout rate    --drop path: Stochastic Depth,        refer to https://arxiv.org/abs/1603.09382    --use_layer_scale, --layer_scale_init_value: LayerScale,        refer to https://arxiv.org/abs/2103.17239    """    def __init__(self, dim, pool_size=3, mlp_ratio=4.,                 act_layer=nn.GELU, norm_layer=GroupNorm,                 drop=0., drop_path=0.,                 use_layer_scale=True, layer_scale_init_value=1e-5):        super().__init__()        self.norm1 = norm_layer(dim)        # self.token_mixer = Pooling(pool_size=pool_size)        self.token_mixer = nn.Sequential(            nn.Conv2d(in_channels=dim, out_channels=dim * 2, kernel_size=1, padding=1 // 2),            nn.Conv2d(in_channels=dim * 2, out_channels=dim * 2, kernel_size=7, padding=7 // 2),            nn.Conv2d(in_channels=dim * 2, out_channels=dim, kernel_size=1, padding=1 // 2),        )        # self.conv1 = nn.Conv2d(in_channel, in_channel, kernel_size=kernel_size, padding=kernel_size // 2)        # self.conv2 = nn.Conv2d(in_channel, dim, kernel_size=1)        self.norm2 = norm_layer(dim)        mlp_hidden_dim = int(dim * mlp_ratio)        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)        # The following two techniques are useful to train deep PoolFormers.        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()        self.use_layer_scale = use_layer_scale        if use_layer_scale:            self.layer_scale_1 = nn.Parameter(layer_scale_init_value * torch.ones((dim)), requires_grad=True)            self.layer_scale_2 = nn.Parameter(layer_scale_init_value * torch.ones((dim)), requires_grad=True)    def forward(self, x):        if self.use_layer_scale:            x = x + self.drop_path(self.layer_scale_1.unsqueeze(-1).unsqueeze(-1) * self.token_mixer(self.norm1(x)))            x = x + self.drop_path(self.layer_scale_2.unsqueeze(-1).unsqueeze(-1) * self.mlp(self.norm2(x)))        else:            x = x + self.drop_path(self.token_mixer(self.norm1(x)))            x = x + self.drop_path(self.mlp(self.norm2(x)))        return xdef basic_blocks(dim, index, pool_size=3, mlp_ratio=4., act_layer=nn.GELU, norm_layer=GroupNorm, drop_rate=.0,                 drop_path_rate=0., use_layer_scale=True, layer_scale_init_value=1e-5):    """    generate PoolFormer blocks for a stage    return: PoolFormer blocks    """    blocks = []    for block_idx in range(index):        block_dpr = drop_path_rate  # * (block_idx + sum(layers[:index])) / (sum(layers) - 1)        blocks.append(PoolFormerBlock(            dim, pool_size=pool_size, mlp_ratio=mlp_ratio,            act_layer=act_layer, norm_layer=norm_layer,            drop=drop_rate, drop_path=block_dpr,            use_layer_scale=use_layer_scale,            layer_scale_init_value=layer_scale_init_value,        ))    blocks = nn.Sequential(*blocks)    return blocksimport torch.nn.functional as Fclass ResBlocks(nn.Module):    def __init__(self, num_blocks, dim, norm, act, pad_type, use_sn=False):        super(ResBlocks, self).__init__()        self.model = nn.ModuleList()        for i in range(num_blocks):            self.model.append(ResBlock(dim, norm=norm, act=act, pad_type=pad_type, use_sn=use_sn))        self.model = nn.Sequential(*self.model)    def forward(self, x):        return self.model(x)class ResBlock(nn.Module):    def __init__(self, dim, norm='in', act='relu', pad_type='zero', use_sn=False):        super(ResBlock, self).__init__()        self.model = nn.Sequential(Conv2dBlock(dim, dim, 3, 1, 1,                                               norm=norm,                                               act=act,                                               pad_type=pad_type, use_sn=use_sn),                                   Conv2dBlock(dim, dim, 3, 1, 1,                                               norm=norm,                                               act='none',                                               pad_type=pad_type, use_sn=use_sn))    def forward(self, x):        x_org = x        residual = self.model(x)        out = x_org + 0.1 * residual        return outclass ActFirstResBlk(nn.Module):    def __init__(self, dim_in, dim_out, downsample=True):        super(ActFirstResBlk, self).__init__()        self.norm1 = FRN(dim_in)        self.norm2 = FRN(dim_in)        self.conv1 = nn.Conv2d(dim_in, dim_in, 3, 1, 1)        self.conv2 = nn.Conv2d(dim_in, dim_out, 3, 1, 1)        self.downsample = downsample        self.learned_sc = (dim_in != dim_out)        if self.learned_sc:            self.conv1x1 = nn.Conv2d(dim_in, dim_out, 1, 1, 0, bias=False)    def _shortcut(self, x):        if self.learned_sc:            x = self.conv1x1(x)        if self.downsample:            x = F.avg_pool2d(x, 2)        return x    def _residual(self, x):        x = self.norm1(x)        x = self.conv1(x)        if self.downsample:            x = F.avg_pool2d(x, 2)        x = self.norm2(x)        x = self.conv2(x)        return x    def forward(self, x):        return torch.rsqrt(torch.tensor(2.0)) * self._shortcut(x) + torch.rsqrt(torch.tensor(2.0)) * self._residual(x)class LinearBlock(nn.Module):    def __init__(self, in_dim, out_dim, norm='none', act='relu', use_sn=False):        super(LinearBlock, self).__init__()        use_bias = True        self.fc = nn.Linear(in_dim, out_dim, bias=use_bias)        if use_sn:            self.fc = nn.utils.spectral_norm(self.fc)        # initialize normalization        norm_dim = out_dim        if norm == 'bn':            self.norm = nn.BatchNorm1d(norm_dim)        elif norm == 'in':            self.norm = nn.InstanceNorm1d(norm_dim)        elif norm == 'none':            self.norm = None        else:            assert 0, "Unsupported normalization: {}".format(norm)        # initialize activation        if act == 'relu':            self.activation = nn.ReLU(inplace=True)        elif act == 'lrelu':            self.activation = nn.LeakyReLU(0.2, inplace=True)        elif act == 'tanh':            self.activation = nn.Tanh()        elif act == 'none':            self.activation = None        else:            assert 0, "Unsupported activation: {}".format(act)    def forward(self, x):        out = self.fc(x)        if self.norm:            out = self.norm(out)        if self.activation:            out = self.activation(out)        return outclass Conv2dBlock(nn.Module):    def __init__(self, in_dim, out_dim, ks, st, padding=0,                 norm='none', act='relu', pad_type='zero',                 use_bias=True, use_sn=False):        super(Conv2dBlock, self).__init__()        self.use_bias = use_bias        # initialize padding        if pad_type == 'reflect':            self.pad = nn.ReflectionPad2d(padding)        elif pad_type == 'replicate':            self.pad = nn.ReplicationPad2d(padding)        elif pad_type == 'zero':            self.pad = nn.ZeroPad2d(padding)        else:            assert 0, "Unsupported padding type: {}".format(pad_type)        # initialize normalization        norm_dim = out_dim        if norm == 'bn':            self.norm = nn.BatchNorm2d(norm_dim)        elif norm == 'in':            self.norm = nn.InstanceNorm2d(norm_dim)        elif norm == 'adain':            self.norm = AdaIN2d(norm_dim)        elif norm == 'none':            self.norm = None        else:            assert 0, "Unsupported normalization: {}".format(norm)        # initialize activation        if act == 'relu':            self.activation = nn.ReLU(inplace=True)        elif act == 'lrelu':            self.activation = nn.LeakyReLU(0.2, inplace=True)        elif act == 'tanh':            self.activation = nn.Tanh()        elif act == 'none':            self.activation = None        else:            assert 0, "Unsupported activation: {}".format(act)        self.conv = nn.Conv2d(in_dim, out_dim, ks, st, bias=self.use_bias)        if use_sn:            self.conv = nn.utils.spectral_norm(self.conv)    def forward(self, x):        x = self.conv(self.pad(x))        if self.norm:            x = self.norm(x)        if self.activation:            x = self.activation(x)        return xclass FRN(nn.Module):    def __init__(self, num_features, eps=1e-6):        super(FRN, self).__init__()        self.tau = nn.Parameter(torch.zeros(1, num_features, 1, 1))        self.gamma = nn.Parameter(torch.ones(1, num_features, 1, 1))        self.beta = nn.Parameter(torch.zeros(1, num_features, 1, 1))        self.eps = eps    def forward(self, x):        x = x * torch.rsqrt(torch.mean(x ** 2, dim=[2, 3], keepdim=True) + self.eps)        return torch.max(self.gamma * x + self.beta, self.tau)class AdaIN2d(nn.Module):    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=False, track_running_stats=True):        super(AdaIN2d, self).__init__()        self.num_features = num_features        self.eps = eps        self.momentum = momentum        self.affine = affine        self.track_running_stats = track_running_stats        if self.affine:            self.weight = nn.Parameter(torch.Tensor(num_features))            self.bias = nn.Parameter(torch.Tensor(num_features))        else:            self.weight = None            self.bias = None        if self.track_running_stats:            self.register_buffer('running_mean', torch.zeros(num_features))            self.register_buffer('running_var', torch.ones(num_features))        else:            self.register_buffer('running_mean', None)            self.register_buffer('running_var', None)    def forward(self, x):        assert self.weight is not None and self.bias is not None, "AdaIN params are None"        N, C, H, W = x.size()        running_mean = self.running_mean.repeat(N)        running_var = self.running_var.repeat(N)        x_ = x.contiguous().view(1, N * C, H * W)        normed = F.batch_norm(x_, running_mean, running_var,                              self.weight, self.bias,                              True, self.momentum, self.eps)        return normed.view(N, C, H, W)    def __repr__(self):        return self.__class__.__name__ + '(num_features=' + str(self.num_features) + ')'import torchvisionclass DeformableConv2d(nn.Module):    def __init__(self,                 in_channels,                 out_channels,                 kernel_size=3,                 stride=1,                 padding=3 // 2,                 bias=False):        super(DeformableConv2d, self).__init__()        self.padding = padding        self.offset_conv = nn.Conv2d(in_channels,                                     2 * kernel_size * kernel_size,                                     kernel_size=kernel_size,                                     stride=stride,                                     padding=self.padding,                                     bias=True)        nn.init.constant_(self.offset_conv.weight, 0.)        nn.init.constant_(self.offset_conv.bias, 0.)        self.modulator_conv = nn.Conv2d(in_channels,                                        1 * kernel_size * kernel_size,                                        kernel_size=kernel_size,                                        stride=stride,                                        padding=self.padding,                                        bias=True)        nn.init.constant_(self.modulator_conv.weight, 0.)        nn.init.constant_(self.modulator_conv.bias, 0.)        self.regular_conv = nn.Conv2d(in_channels=out_channels,                                      out_channels=out_channels,                                      kernel_size=kernel_size,                                      stride=stride,                                      padding=self.padding,                                      bias=bias)    def forward(self, x,y):        h, w = x.shape[2:]        max_offset = max(h, w) / 4.        offset = self.offset_conv(x).clamp(-max_offset, max_offset)        modulator = 2. * torch.sigmoid(self.modulator_conv(x))        x = torchvision.ops.deform_conv2d(input=y,                                          offset=offset,                                          weight=self.regular_conv.weight,                                          bias=self.regular_conv.bias,                                          padding=self.padding,                                          mask=modulator                                          )        return xclass DeformableConv2d1(nn.Module):    def __init__(self,                 in_channels,                 out_channels,                 kernel_size=7,                 stride=1,                 padding=7 // 2,                 bias=False):        super(DeformableConv2d1, self).__init__()        self.padding = padding        self.offset_conv = nn.Conv2d(in_channels,                                     2 * kernel_size * kernel_size,                                     kernel_size=kernel_size,                                     stride=stride,                                     padding=self.padding,                                     bias=True)        nn.init.constant_(self.offset_conv.weight, 0.)        nn.init.constant_(self.offset_conv.bias, 0.)        self.modulator_conv = nn.Conv2d(in_channels,                                        1 * kernel_size * kernel_size,                                        kernel_size=kernel_size,                                        stride=stride,                                        padding=self.padding,                                        bias=True)        nn.init.constant_(self.modulator_conv.weight, 0.)        nn.init.constant_(self.modulator_conv.bias, 0.)        self.regular_conv = nn.Conv2d(in_channels=in_channels,                                      out_channels=out_channels,                                      kernel_size=kernel_size,                                      stride=stride,                                      padding=self.padding,                                      bias=bias)    def forward(self, x):        h, w = x.shape[2:]        max_offset = max(h, w) / 4.        offset = self.offset_conv(x).clamp(-max_offset, max_offset)        modulator = 2. * torch.sigmoid(self.modulator_conv(x))        #print(offset.shape,modulator.shape,self.regular_conv.weight.shape,self.regular_conv.bias)        x = torchvision.ops.deform_conv2d(input= x,                                          offset=offset,                                          weight=self.regular_conv.weight,                                          bias=self.regular_conv.bias,                                          padding=self.padding,                                          mask=modulator                                          )        return xclass SkipConnection(nn.Module):    def __init__(self, in_channels, out_channels):        super(SkipConnection, self).__init__()        self.input = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=True)        self.a = nn.Conv2d(out_channels, out_channels, kernel_size=1, stride=1, bias=True)        self.b = nn.Conv2d(out_channels, out_channels, kernel_size=1, stride=1, bias=True)    def forward(self, x, y):        input = self.input(y)        a = self.a(input)        b = self.b(input)        out = a * x + b        return outclass Generator(nn.Module):    def __init__(self, image_size=256):        super().__init__()        self.down1 = nn.Sequential(            nn.LeakyReLU(0.2),            nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=2, padding=1),            FRN(12),            nn.LeakyReLU(0.2),            DeformableConv2d1(12,12),            FRN(12),            ) #128*128        self.down2 = nn.Sequential(            nn.LeakyReLU(0.2),            nn.Conv2d(in_channels=12, out_channels=48, kernel_size=3, stride=2, padding=1),            FRN(48),            nn.LeakyReLU(0.2),            DeformableConv2d1(48, 48),            FRN(48),            )#64*64        self.down3 = nn.Sequential(            nn.LeakyReLU(0.2),            nn.Conv2d(in_channels=48, out_channels=192, kernel_size=3, stride=2, padding=1),            FRN(192),            nn.LeakyReLU(0.2),            DeformableConv2d1(192, 192),            FRN(192),            )#32*32        self.down4 = nn.Sequential(            nn.LeakyReLU(0.2),            nn.Conv2d(in_channels=192, out_channels=768, kernel_size=3, stride=2, padding=1),            FRN(768),            nn.LeakyReLU(0.2),            DeformableConv2d1(768, 768),            FRN(768),        )  # 32*32        self.up1 = nn.Sequential(            ResBlocks(num_blocks=3, dim=768, norm="in", act="relu", pad_type="zero"),            nn.Upsample(scale_factor=2),            nn.ReLU(),            nn.Conv2d(in_channels=768, out_channels=192, kernel_size=5, stride=1, padding=2),            FRN(192),             )        self.up2 = nn.Sequential(            nn.Upsample(scale_factor=2),            nn.ReLU(),            nn.Conv2d(in_channels=384, out_channels=48, kernel_size=5, stride=1, padding=2),            FRN(48),        )        self.up3 = nn.Sequential(            nn.Upsample(scale_factor=2),            nn.ReLU(),            nn.Conv2d(in_channels=96, out_channels=12, kernel_size=5, stride=1, padding=2),            FRN(12),        )        self.up4 = nn.Sequential(            nn.Upsample(scale_factor=2),            nn.ReLU(),            nn.Conv2d(in_channels=24, out_channels=3, kernel_size=5, stride=1, padding=2),            nn.Tanh()        )        self.dcn1=DeformableConv2d(384, 192)        self.dcn2=DeformableConv2d(96, 48)        self.dcn3=DeformableConv2d(24, 12)    def forward(self, x, style_or_label=None):       d1= self.down1(x)       d2 = self.down2(d1)       d3 = self.down3(d2)       d4 = self.down4(d3)       up1=self.up1(d4)       deformable_concat = torch.cat((up1, d3), dim=1)       concat_pre1 = self.dcn1(deformable_concat,d3)       up1 = torch.cat([concat_pre1, up1], dim=1)       up2 = self.up2(up1)       deformable_concat = torch.cat((up2, d2), dim=1)       concat_pre = self.dcn2(deformable_concat, d2)       up2 = torch.cat([concat_pre, up2], dim=1)       up3 = self.up3(up2)       deformable_concat = torch.cat((up3, d1), dim=1)       concat_pre = self.dcn3(deformable_concat, d1)       up3_1 = torch.cat([concat_pre, up3], dim=1)       out = self.up4(up3_1)       return out, d4,up3_1,deformable_concatclass Zi2ZiModel:    def __init__(self, input_nc=3, embedding_num=40, embedding_dim=128,                 ngf=64, ndf=64,                 Lconst_penalty=15, Lcategory_penalty=1, L1_penalty=100,                 schedule=10, lr=0.001, gpu_ids=None, save_dir='.', is_training=True):        self.gpu_ids = gpu_ids        self.is_training = is_training        self.lr=lr        self.L1_penalty=L1_penalty    def set_input(self, labels, real_A, real_B):        if self.gpu_ids:            self.real_A = real_A.to(self.gpu_ids[0])            self.real_B = real_B.to(self.gpu_ids[0])            self.labels = labels.to(self.gpu_ids[0])        else:            self.real_A = real_A            self.real_B = real_B            self.labels = labels    def setup(self):        # self.netG = UNetGenerator(input_nc=self.input_nc, embedding_num=self.embedding_num, embedding_dim=self.embedding_dim,        #                             ngf=self.ngf, use_dropout=self.use_dropout)        self.netG = Generator()        init_net(self.netG, gpu_ids=self.gpu_ids)        self.optimizer_G = torch.optim.Adam(self.netG.parameters(), lr=self.lr, betas=(0.5, 0.999))        self.scheduler_G = StepLR(self.optimizer_G, step_size=1, gamma=0.5)        self.l1_loss = nn.L1Loss()        self.mse = nn.MSELoss()        self.sigmoid = nn.Sigmoid()        if self.gpu_ids:            self.l1_loss.cuda()            self.mse.cuda()            self.sigmoid.cuda()        if self.is_training:            self.netG.train()        else:            self.netG.eval()    def optimize_parameters(self):        self.forward()  # compute fake images: G(A)        self.optimizer_G.zero_grad()  # set G's gradients to zero        l1_loss = self.L1_penalty * self.l1_loss(self.fake_B, self.real_B)        print(l1_loss.data)        l1_loss.backward()        self.optimizer_G.step()  # udpate G's weights    def save(self,batch, basename):        tensor_to_plot = torch.cat([self.fake_B, self.real_A,self.real_B], 3)        vutils.save_image(tensor_to_plot, basename + "_generate.png")        torch.save(self.up3_1, basename + "_generateUp3_1.pt")        torch.save(self.up3, basename + "_generateUp3.pt")    def forward(self):        # generate fake_B        self.fake_B,self.encoded_real_A,self.up3_1,self.up3 = self.netG(self.real_A, self.labels)if __name__ == '__main__':    # m = Generator()    # input = torch.randn(1, 3, 256, 256)    # m(input)    from data import DatasetFromObj    from torch.utils.data import DataLoader    train_dataset = DatasetFromObj('/Users/liuyu/Desktop/zi2zi-pytorch-master-2/dataset/train.obj')    dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True, drop_last=True)    sample_dir="/Users/liuyu/Desktop/zi2zi-pytorch-master-2/sample/"    model = Zi2ZiModel()    model.setup()    for epoch in range(0, 1000):        for i, batch in enumerate(dataloader):            model.set_input(batch[0], batch[2], batch[1])            model.optimize_parameters()            if i % 20 == 0:                model.save(batch, os.path.join(sample_dir, "sample_{}_{}".format(epoch, i)))